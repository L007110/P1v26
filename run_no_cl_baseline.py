import torch
import torch.optim as optim
import Parameters
import Main
import GNNModel
from Topology import formulate_global_list_dqn
from logger import global_logger


def run_no_cl_baseline():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ å¯åŠ¨ No-CL åŸºå‡†è®­ç»ƒ (Hard Mode Direct Training)")
    print(f"ğŸ“ ç›®æ ‡å¯†åº¦: N=140 (ç›´æ¥è®­ç»ƒï¼Œæ— è¯¾ç¨‹)")
    print(f"â±ï¸ æ€» Epochs: 2150 (ä¸ CL ä¿æŒä¸€è‡´)")

    # 1. è®¾ç½®å‚æ•°
    Parameters.USE_GNN_ENHANCEMENT = True
    Parameters.GNN_ARCH = "HYBRID"
    Parameters.SCENE_SCALE_X = 1200
    Parameters.SCENE_SCALE_Y = 1200

    # å…³é”®ï¼šç›´æ¥è®¾å®šä¸ºæœ€éš¾éš¾åº¦ï¼Œä¸”ä¸æ›´æ”¹
    Parameters.TRAINING_VEHICLE_TARGET = 140
    Parameters.NUM_VEHICLES = 140

    # æ€» Epochs ç­‰äº CL ç´¯åŠ çš„æ€»å’Œ
    Parameters.RL_N_EPOCHS = 2150

    # ä¸ºäº†åŒºåˆ†æ—¥å¿—
    Parameters.ABLATION_SUFFIX = "_NoCL_Baseline_N140"

    # 2. åˆå§‹åŒ–ç¯å¢ƒ
    formulate_global_list_dqn(Parameters.global_dqn_list, device)

    # 3. åˆå§‹åŒ–æ¨¡å‹
    GNNModel.global_gnn_model = GNNModel.EnhancedHeteroGNN(node_feature_dim=12, hidden_dim=64).to(device)
    GNNModel.global_target_gnn_model = GNNModel.EnhancedHeteroGNN(node_feature_dim=12, hidden_dim=64).to(device)
    GNNModel.update_target_gnn()

    # 4. ä¼˜åŒ–å™¨ (ä½¿ç”¨ä¸€ä¸ªæŠ˜ä¸­çš„å­¦ä¹ ç‡ï¼Œæˆ–è€… CL æœ€åé˜¶æ®µçš„å­¦ä¹ ç‡)
    gnn_optimizer = optim.Adam(GNNModel.global_gnn_model.parameters(), lr=0.0003)

    # 5. å¼€å§‹è®­ç»ƒ
    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥è°ƒç”¨ Main.rlï¼Œä¸è¦æŒ‚è½½ run_smart_curriculum... ä¸­çš„ density_mixer
    try:
        Main.rl(gnn_optimizer=gnn_optimizer, device=device)

        # ä¿å­˜æ¨¡å‹
        save_name = "model_NoCL_Baseline_N140.pt"
        torch.save(GNNModel.global_gnn_model.state_dict(), save_name)
        print(f"âœ… No-CL æ¨¡å‹å·²ä¿å­˜: {save_name}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_no_cl_baseline()